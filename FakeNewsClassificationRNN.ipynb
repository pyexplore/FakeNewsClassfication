{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This notebook covers the creation, execution and evaluation of an Recurrent Neural Long Short Term Memory Network model.\n",
    "The steps followed in this notebook are as follows\n",
    "1. Load the proprocessed data to a dataframe from the google drive(as this notebook used google Colab).\n",
    "2. Tokenize the news articles using NLTK tokenizer and thereafter apply padding on the encoding of the news article.\n",
    "3. Create and fit the model on train set.\n",
    "4. Predict the validation and test set. Generate accuracy score for validation and test predictions.\n",
    "5. Save the model for future use in model Evaluation notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6202,
     "status": "ok",
     "timestamp": 1690610087188,
     "user": {
      "displayName": "Midhun Mathew",
      "userId": "10881148303276454761"
     },
     "user_tz": 420
    },
    "id": "lVkwo--RqFDT",
    "outputId": "a23c208d-fffb-4155-8d8d-b903609673dd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "# Importing required packages\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import re\n",
    "\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk import word_tokenize\n",
    "\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.decomposition import PCA\n",
    "import pickle\n",
    "\n",
    "# import keras\n",
    "from tensorflow.keras.preprocessing.text import one_hot, Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Embedding, Input, LSTM, Conv1D, MaxPool1D, Bidirectional\n",
    "from tensorflow.keras.models import Model\n",
    "#from jupyterthemes import jtplot\n",
    "#jtplot.style(theme='monokai', context='notebook', ticks=True, grid=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21880,
     "status": "ok",
     "timestamp": 1690610110488,
     "user": {
      "displayName": "Midhun Mathew",
      "userId": "10881148303276454761"
     },
     "user_tz": 420
    },
    "id": "yr6_m--IsMIF",
    "outputId": "f4133ad4-95ae-4c7b-dd56-d3d408831548"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "# Mouting the google drive to google collaboratory\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gduFH7nb7YHA"
   },
   "source": [
    "Before the modelling lets load the preprocessed news data into train, test and validation dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 5897,
     "status": "ok",
     "timestamp": 1690610120267,
     "user": {
      "displayName": "Midhun Mathew",
      "userId": "10881148303276454761"
     },
     "user_tz": 420
    },
    "id": "3p7RK7IdBG4Q"
   },
   "outputs": [],
   "source": [
    "# Read the CSV file from the preprocessing step\n",
    "news_df = pd.read_csv('/content/gdrive/MyDrive/Colab Notebooks/train_processed.csv', encoding='UTF-8')\n",
    "\n",
    "# Split the data into train, validation, and test sets\n",
    "#train_df, temp_df = train_test_split(news_df, test_size=0.2, random_state=42)\n",
    "#val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 774,
     "status": "ok",
     "timestamp": 1690610122313,
     "user": {
      "displayName": "Midhun Mathew",
      "userId": "10881148303276454761"
     },
     "user_tz": 420
    },
    "id": "iOg6sABa83rz",
    "outputId": "76eddee6-3ffa-42c6-ed7c-6b5f2149e525"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "News df Shape: (20758, 6)\n"
     ]
    }
   ],
   "source": [
    "print(\"News df Shape:\", news_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 743,
     "status": "ok",
     "timestamp": 1690610125354,
     "user": {
      "displayName": "Midhun Mathew",
      "userId": "10881148303276454761"
     },
     "user_tz": 420
    },
    "id": "YTuAVVtwtid2"
   },
   "outputs": [],
   "source": [
    "# Split the data into train, validation, and test sets\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(news_df['clean_joined'], news_df['label'], test_size=0.1, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_trainval, y_trainval, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 622,
     "status": "ok",
     "timestamp": 1690610128202,
     "user": {
      "displayName": "Midhun Mathew",
      "userId": "10881148303276454761"
     },
     "user_tz": 420
    },
    "id": "5-ZwA5-W8isP",
    "outputId": "59b378b4-95ce-4fc7-e95b-4a0fb280d6ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data Shape: (14945,)\n",
      "Validation Data Shape: (3737,)\n",
      "Test Data Shape: (2076,)\n"
     ]
    }
   ],
   "source": [
    "# Print the shape of the dataframes\n",
    "print(\"Train Data Shape:\", X_train.shape)\n",
    "print(\"Validation Data Shape:\", X_val.shape)\n",
    "print(\"Test Data Shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BtQpxlth9Lgd"
   },
   "source": [
    "Next step is to find the total words in the entire news_df dataframe and maximum words in a news article. This is useful for word vectorizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 1997,
     "status": "ok",
     "timestamp": 1690610132856,
     "user": {
      "displayName": "Midhun Mathew",
      "userId": "10881148303276454761"
     },
     "user_tz": 420
    },
    "id": "IhmxAWZ7vWr4"
   },
   "outputs": [],
   "source": [
    "# Obtain the total words present in the dataset\n",
    "list_of_words = []\n",
    "for i in news_df['clean_joined']:\n",
    "    for j in i.split():\n",
    "        list_of_words.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 648,
     "status": "ok",
     "timestamp": 1690610134546,
     "user": {
      "displayName": "Midhun Mathew",
      "userId": "10881148303276454761"
     },
     "user_tz": 420
    },
    "id": "SHByALeqv2nB",
    "outputId": "80072d26-65a8-4d4d-e69d-653324affce8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7466427"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Totalnumber of words in the news dataframe.\n",
    "len(list_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 28225,
     "status": "ok",
     "timestamp": 1690610165739,
     "user": {
      "displayName": "Midhun Mathew",
      "userId": "10881148303276454761"
     },
     "user_tz": 420
    },
    "id": "AtCAOOUzfi7a",
    "outputId": "08d29d4f-691d-4bde-ecaa-b426539c4c16"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum number of words in any document is = 13775\n"
     ]
    }
   ],
   "source": [
    "# length of maximum document will be needed to create word embeddings\n",
    "maxlen = -1\n",
    "for doc in news_df.clean_joined:\n",
    "    tokens = nltk.word_tokenize(doc)\n",
    "    if(maxlen<len(tokens)):\n",
    "        maxlen = len(tokens)\n",
    "print(\"The maximum number of words in any document is =\", maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1320,
     "status": "ok",
     "timestamp": 1690610170865,
     "user": {
      "displayName": "Midhun Mathew",
      "userId": "10881148303276454761"
     },
     "user_tz": 420
    },
    "id": "yXmoFbzUvj63",
    "outputId": "22bd4ca1-450c-413c-a724-4835b7b5083b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "170784"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Obtain the total number of unique words\n",
    "total_words = len(list(set(list_of_words)))\n",
    "total_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ZlT1GuE_0Oa"
   },
   "source": [
    "NLTK Tokenizer is used to embed words in the news article and train, test and validation sequences are created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 9431,
     "status": "ok",
     "timestamp": 1690610182174,
     "user": {
      "displayName": "Midhun Mathew",
      "userId": "10881148303276454761"
     },
     "user_tz": 420
    },
    "id": "iOAQ32gjb6dk"
   },
   "outputs": [],
   "source": [
    "# Create a tokenizer to tokenize the words and create sequences of tokenized words\n",
    "tokenizer = Tokenizer(num_words = total_words)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "train_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "val_sequences = tokenizer.texts_to_sequences(X_val)\n",
    "test_sequences = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 313,
     "status": "ok",
     "timestamp": 1690610186539,
     "user": {
      "displayName": "Midhun Mathew",
      "userId": "10881148303276454761"
     },
     "user_tz": 420
    },
    "id": "QZ-CXYKgeFtR",
    "outputId": "f62fe3cc-cff4-41b2-c940-86ab06acdc85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The encoding for document\n",
      " house aide comey letter jason chaffetz tweeted house aide comey letter jason chaffetz tweeted darrell lucus october subscribe jason chaffetz stump american fork utah image courtesy michael jolley available creative commons license apologies keith olbermann doubt worst person world week director james comey according house democratic aide looks like know second worst person turns comey sent infamous letter announcing looking emails related hillary clinton email server ranking democrats relevant committees hear comey tweet republican committee chairmen know comey notified republican chairmen democratic ranking members house intelligence judiciary oversight committees agency reviewing emails recently discovered order contained classified information long letter went oversight committee chairman jason chaffetz political world ablaze tweet informed learned existence emails appear pertinent investigation case reopened jason chaffetz jasoninthehouse october course know case comey actually saying reviewing emails light unrelated case know anthony weiner sexting teenager apparently little things facts matter chaffetz utah republican vowed initiate raft investigations hillary wins years worth possibly entire term worth apparently chaffetz thought work resulting tweet briefly roiled nation cooler heads realized according senior house democratic aide misreading letter chaffetz sins aide told shareblue boss democrats know comey letter time checked twitter democratic ranking members relevant committees receive comey letter republican chairmen fact democratic ranking members receive chairman oversight government reform committee jason chaffetz tweeted public right director tells chaffetz committee chairmen major development potentially politically explosive investigation chaffetz colleagues courtesy democratic counterparts know instead according aide twitter talk daily comey provided advance notice letter chaffetz republicans giving time turn spin machine good theater suggests case suggests comey grossly incompetent tone deaf suggest chaffetz acting makes burton darrell issa look like models responsibility bipartisanship decency notify ranking member elijah cummings explosive trample basic standards fairness know granted likely chaffetz answer sits ridiculously republican district anchored provo orem cook partisan voting index gave mitt romney punishing percent vote republican house leadership given support chaffetz planned fishing expedition mean turn lights textbook example house republican control second worst person world darrell lucus darrell graduate university north carolina considers journalist school attempt turn member religious right college succeeded turning religious right worst nightmare charismatic christian unapologetic liberal desire stand scared silence increased survived abusive year marriage know daily christian follow twitter darrelllucus connect facebook click darrell mello yello connect \n",
      " is :  [9318, 5962, 1236, 474, 5962, 20, 15, 64, 5750, 4958, 10636, 456, 1862, 8448, 62148, 10636, 785, 1116, 313, 148, 7417, 4959, 4339, 780, 1195, 179, 234, 31, 2, 150, 359, 9318, 1813, 126, 1493, 122, 362, 9318, 1726, 1236, 474, 90, 21, 9318, 291, 3981, 242, 135, 1577, 2697, 208, 456, 64, 1136, 18, 242, 13, 4, 74, 9318, 965, 5398, 101, 510, 8448, 38, 180, 352, 2477, 242, 4, 133, 40, 26758, 3289, 74, 9318, 3905, 575, 225, 456, 5750, 2698, 223, 29, 789, 503, 13, 4, 1887, 431, 207, 162, 43, 1312, 12850, 1307, 15422, 6342, 2629, 156, 1798, 8448, 1236, 474, 24, 250, 2200, 701, 313, 394, 244, 557, 10, 2571, 29350, 220, 244, 1255, 99, 914, 172, 914, 6311, 5750, 29, 159, 1655, 99, 914, 2991, 6311, 197, 850, 2747, 243, 2478, 3608, 1655, 172, 914, 4, 81371, 14083, 5194, 2393, 6758, 2761, 7947, 815, 1114, 4, 850, 607, 2858, 8, 1257, 850, 2204, 8448, 394, 2035, 4411, 592, 244, 225, 6, 16, 6649, 6649, 6101, 2267, 29, 2673, 24, 1039, 5750, 29, 8448, 1, 2477, 1940, 46, 313, 4, 8448, 3310, 8669, 104, 5750, 1693, 1, 1179, 5, 196, 24, 8117, 3707, 1426, 8448, 18, 618, 5750, 465, 275, 1477, 8448, 29, 1678, 339, 5750, 159, 429, 97, 127, 7584, 64, 850, 622, 2378, 29, 176, 259, 1357, 22140, 320, 474, 404, 244, 685, 212, 4, 45894, 2653, 2981, 22141, 276, 244, 167, 164, 78, 8448, 29, 321, 829, 5750, 758, 1701, 8448, 758, 78, 4264, 3299, 74, 2, 22, 557, 78, 2, 5750, 58, 29, 16989, 2505, 695, 16, 2821, 2030, 1236, 1834, 730, 16, 8448, 29, 1996, 1996, 7, 175, 20196, 325, 2216, 22, 474, 321, 819, 1196, 4, 455, 2477, 74, 219, 2, 4299, 1236, 321, 64, 9228, 5750, 2698, 29, 400, 37, 223, 1384, 2, 1009, 16990, 2673, 2402, 3, 881, 101, 2982, 8448, 136, 1236, 286, 1039, 9318, 1384, 9318, 265, 21, 77, 474, 669, 245, 1257, 3232, 26, 9318, 127, 671, 266, 18643, 76, 123, 671, 9701, 2710, 266, 113, 85, 2146, 5750, 29, 37, 13038, 4, 474, 1919, 2427, 52202, 123, 4, 818, 276, 769, 3290, 76, 123, 495, 6198, 172, 769, 3290, 7236, 474, 10197, 8448, 474, 321, 987, 21417, 5686, 9318, 2427, 508, 78, 8, 1808, 741, 37, 5750, 109, 2002, 3796, 9318, 402, 3232, 1332, 9318, 15995, 1269, 66, 815, 5996, 730, 3915, 2686, 101, 8174, 251, 5783, 9318, 232, 29351, 3343, 1493, 313, 212, 612, 37901, 491, 8448, 1330, 2812, 9318, 431, 279, 5751, 30, 163, 28, 315, 474, 2181, 4, 685, 31, 2, 460, 163, 28, 16991, 72, 860, 191, 3, 1650, 495, 78, 3232, 9782, 29, 4281, 318, 1357, 22140, 2443, 2761, 2182, 4938, 5889, 57, 474, 9318, 245, 354, 5750, 21418, 456, 8448, 8448, 252, 910, 9318, 5750, 29, 929, 4640, 404, 5890, 1039, 313, 196, 848, 42, 2653, 2981, 5, 150, 1399, 21417, 6030, 491, 5, 122, 8058, 9318, 400, 137, 613, 8908, 1293, 3535, 15996, 62149, 2981, 4640, 51, 326, 136, 6957, 313, 4641, 782, 1384, 1562, 695, 16, 3440, 2946, 196, 16629, 7279, 8909, 296, 1295, 33, 685, 2216, 8737, 44, 758, 1009, 5, 99, 799, 2443, 29, 9318, 4825, 13039, 402, 3232, 8448, 9318, 9880, 3, 29, 3494, 13, 4, 21419, 339, 5647, 3201, 12851, 32839, 16314, 474, 16992, 4, 2653, 2981, 18644, 9318, 7044, 3981, 123, 1592, 88, 5, 508, 78, 9063, 6682, 2991, 5040, 404, 2382, 3300, 29, 474, 321, 2303, 345, 1834, 331, 9318, 4385, 47, 5, 455, 802, 1384, 9318, 13837, 208, 456, 5750, 29, 1714, 220, 1384, 503, 245, 557, 9318, 62, 6139, 196, 11133, 445, 2981, 42, 2981, 5, 74, 404, 326, 29, 313, 4641, 782, 164, 8737, 100, 4084, 9318, 14320, 313, 196, 1057, 508, 78, 2056, 583, 8448, 77, 474, 6281, 458, 10310, 9318, 17770, 438, 563, 21, 1267, 1149, 82, 32839, 1039, 3823, 2383, 2395, 4, 488, 474, 321, 2821, 2030, 32839, 67, 9318, 3968, 403, 300, 1432, 802, 37, 5750, 95, 475, 163, 82, 9318, 400, 1743, 731, 122, 1307, 3, 1042, 1397, 8448, 3135, 244, 31, 2, 480, 7, 4168, 8, 204, 4697, 10, 918, 4068, 46, 81372, 5750, 29, 29, 8448, 156, 21, 1516, 400, 1301, 5750, 1186, 44, 15, 4265, 21, 3300, 37, 804, 756, 2748, 113, 294, 5750, 8448, 1538, 481, 474, 321, 456, 1798, 4906, 1001, 474, 41341, 4, 1809, 31, 2, 2821, 2030, 3573, 4]\n"
     ]
    }
   ],
   "source": [
    "print(\"The encoding for document\\n\",news_df.clean_joined[0],\"\\n is : \",train_sequences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 594,
     "status": "ok",
     "timestamp": 1690610190233,
     "user": {
      "displayName": "Midhun Mathew",
      "userId": "10881148303276454761"
     },
     "user_tz": 420
    },
    "id": "0ZXq-8FEeQQ3"
   },
   "outputs": [],
   "source": [
    "# Add padding can be up to 13775 i.e the max number of words in a news article. We selected maxlen = 4000 as average\n",
    "# word length in the news article is found to be 4000.\n",
    "padded_train = pad_sequences(train_sequences,maxlen = 4000, padding = 'post', truncating = 'post')\n",
    "padded_val = pad_sequences(val_sequences,maxlen = 4000,  padding = 'post', truncating = 'post')\n",
    "padded_test = pad_sequences(test_sequences,maxlen = 4000,  padding = 'post', truncating = 'post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PQ5qsmVfBDfZ"
   },
   "source": [
    "In the next step, a LSTM RNN model is trained to fit the train dataframe.\n",
    "Below are the steps\n",
    "1. Importing Sequential model from Keras.\n",
    "2. Adding an Embedding layer to the model. This layer is often used to process\n",
    "words or other values that have a huge number of categories and can be\n",
    "represented as dense vectors. 'output_dim = 128' specifies the size of\n",
    "the vector space in which words will be embedded. It defines the size of the\n",
    "output vectors from this layer for each word.\n",
    "3. Adding a Bidirectional wrapper for LSTM (Long Short Term Memory) which is a\n",
    "type of Recurrent Neural Network (RNN). The LSTM will learn how to predict the\n",
    "next word based on the previous one it has seen. The use of Bidirectional is\n",
    "to make the LSTM \"look\" backwards in the input sequence and in theory provide\n",
    "additional context to the model. It will have a total of 128 units or \"cells\".\n",
    "4. Adding a Dense layer (Fully connected layer) where every node in the layer is\n",
    "connected to every node in the preceding layer. The number 128 indicates how\n",
    "many neurons are in this layer. The 'relu' activation function is used.\n",
    "5. Adding another Dense layer with 1 neuron as it is usually the case for binary\n",
    "classification problems (as suggested by 'sigmoid' activation). Sigmoid activation\n",
    "function outputs a value between 0 and 1 which can be treated as a probability for\n",
    "the binary classes.\n",
    "6. The compile method is used to configure the learning process before training the\n",
    "model. It receives three arguments. An optimizer ('adam' in this case), a loss\n",
    "function ('binary_crossentropy' which is suitable for binary classification),\n",
    "and a list of metrics ('acc' stands for accuracy).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5214,
     "status": "ok",
     "timestamp": 1690610200398,
     "user": {
      "displayName": "Midhun Mathew",
      "userId": "10881148303276454761"
     },
     "user_tz": 420
    },
    "id": "QapzTJd8inN0",
    "outputId": "01462f7c-2156-4cdf-8d16-30350be157f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 128)         21860352  \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 256)              263168    \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 22,156,545\n",
      "Trainable params: 22,156,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Sequential Model\n",
    "model = Sequential()\n",
    "\n",
    "# embeddidng layer\n",
    "model.add(Embedding(total_words, output_dim = 128))\n",
    "\n",
    "\n",
    "# Bi-Directional RNN and LSTM\n",
    "model.add(Bidirectional(LSTM(128)))\n",
    "\n",
    "# Dense layers\n",
    "model.add(Dense(128, activation = 'relu'))\n",
    "model.add(Dense(1,activation= 'sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 268,
     "status": "ok",
     "timestamp": 1690610206394,
     "user": {
      "displayName": "Midhun Mathew",
      "userId": "10881148303276454761"
     },
     "user_tz": 420
    },
    "id": "KZTrarFYi4Bl"
   },
   "outputs": [],
   "source": [
    "# Converting a y_train series to an array.\n",
    "y_train = np.asarray(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 265889,
     "status": "ok",
     "timestamp": 1690610476668,
     "user": {
      "displayName": "Midhun Mathew",
      "userId": "10881148303276454761"
     },
     "user_tz": 420
    },
    "id": "HjBnwhcAi8aa",
    "outputId": "c12f44c2-481b-48f7-bc55-42e8f7a68a6f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "211/211 [==============================] - 111s 470ms/step - loss: 0.1870 - acc: 0.9205 - val_loss: 0.0879 - val_acc: 0.9639\n",
      "Epoch 2/2\n",
      "211/211 [==============================] - 98s 462ms/step - loss: 0.0265 - acc: 0.9931 - val_loss: 0.1133 - val_acc: 0.9572\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7883f7743520>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model\n",
    "model.fit(padded_train, y_train, batch_size = 64, validation_split = 0.1, epochs = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16807,
     "status": "ok",
     "timestamp": 1690610493473,
     "user": {
      "displayName": "Midhun Mathew",
      "userId": "10881148303276454761"
     },
     "user_tz": 420
    },
    "id": "WWnE3wRMjamA",
    "outputId": "6abcbbb2-326e-4f72-fe84-f743b46d3cda"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65/65 [==============================] - 6s 86ms/step\n",
      "117/117 [==============================] - 10s 86ms/step\n"
     ]
    }
   ],
   "source": [
    "# make prediction\n",
    "pred_test = model.predict(padded_test)\n",
    "pred_val = model.predict(padded_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 413,
     "status": "ok",
     "timestamp": 1690610497353,
     "user": {
      "displayName": "Midhun Mathew",
      "userId": "10881148303276454761"
     },
     "user_tz": 420
    },
    "id": "i8EusWGzjbvB"
   },
   "outputs": [],
   "source": [
    "# if the predicted value is >0.5 it is real else it is fake\n",
    "prediction_test = []\n",
    "for i in range(len(pred_test)):\n",
    "    if pred_test[i].item() > 0.5:\n",
    "        prediction_test.append(1)\n",
    "    else:\n",
    "        prediction_test.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 556,
     "status": "ok",
     "timestamp": 1690610500264,
     "user": {
      "displayName": "Midhun Mathew",
      "userId": "10881148303276454761"
     },
     "user_tz": 420
    },
    "id": "Vzq-EgZrjfKG",
    "outputId": "de272e28-8bf0-4a06-cbef-276c129e2bbd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy :  0.9590558766859345\n"
     ]
    }
   ],
   "source": [
    "# getting the accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy = accuracy_score(list(y_test), prediction_test)\n",
    "\n",
    "print(\"Model Accuracy : \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 737,
     "status": "ok",
     "timestamp": 1690610505255,
     "user": {
      "displayName": "Midhun Mathew",
      "userId": "10881148303276454761"
     },
     "user_tz": 420
    },
    "id": "BX9r8-__4f01"
   },
   "outputs": [],
   "source": [
    "# if the predicted value is >0.5 it is real else it is fake\n",
    "prediction_val = []\n",
    "for i in range(len(pred_val)):\n",
    "    if pred_val[i].item() > 0.5:\n",
    "        prediction_val.append(1)\n",
    "    else:\n",
    "        prediction_val.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 709,
     "status": "ok",
     "timestamp": 1690610509527,
     "user": {
      "displayName": "Midhun Mathew",
      "userId": "10881148303276454761"
     },
     "user_tz": 420
    },
    "id": "SUNU-N3P42Kc",
    "outputId": "9e1d76e5-79af-45ed-b324-d5397e431ca9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy :  0.9644099545089644\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(list(y_val), prediction_val)\n",
    "\n",
    "print(\"Model Accuracy : \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "executionInfo": {
     "elapsed": 1787,
     "status": "ok",
     "timestamp": 1690610515518,
     "user": {
      "displayName": "Midhun Mathew",
      "userId": "10881148303276454761"
     },
     "user_tz": 420
    },
    "id": "Zy900u0yNASc"
   },
   "outputs": [],
   "source": [
    "# Save the RNN model\n",
    "\n",
    "with open('rnn_news_classification.pkl', 'wb') as file:\n",
    "    pickle.dump(model, file)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOnZn22PU18Fn3wfjX3ut0U",
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
